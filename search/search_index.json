{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CS-5330 Pattern Recognition and Computer Vision","text":"<p>This repository contains projects developed for CS-5330 Pattern Recognition and Computer Vision course. Each assignment explores fundamental computer vision concepts through practical implementations.</p> <p>Institution: Northeastern University   Focus Areas: Deep learning, Image processing, Augmented Reality, object retrieval, feature extraction, object recognition, camera calibration, </p>"},{"location":"#technologies","title":"Technologies","text":"<ul> <li>OpenCV - Image processing, camera calibration, AR projection</li> <li>C++ - Core implementations for assignments 1-4</li> <li>PyTorch - Deep learning framework for assignment 5</li> <li>Python - Neural network training and testing</li> <li>Classical computer vision algorithms</li> <li>Deep learning architectures (CNNs, transfer learning)</li> </ul>"},{"location":"#projects-overview","title":"Projects Overview","text":""},{"location":"#real-time-filtering","title":"Real-Time Filtering","text":"<p>Real-time image processing with visual effects on live camera feed and static images.</p> <p>Key Features: Negative effect, blur filters, cartoonization, edge detection   Concepts: Color space transformations (RGB, HSV), convolution operations, real-time processing   Demo: </p>"},{"location":"#content-based-image-retrieval-cbir","title":"Content-Based Image Retrieval (CBIR)","text":"<p>Sophisticated image retrieval system using histogram-based features and distance metrics.</p> <p>Key Features: 8+ feature extraction techniques, 6 distance metrics, CSV caching (~100x speedup)   Concepts: Histogram-based features, texture analysis (gradient, Laplacian), distance metrics (SSE, histogram intersection)   Performance: Handles 1000+ image databases efficiently    </p>"},{"location":"#real-time-2d-object-recognition","title":"Real-Time 2D Object Recognition","text":"<p>Real-time object classification system recognizing 17 object types using K-NN.</p> <p>Key Features: Moment-based features, Hu moments, rotation invariance   Concepts: Morphological operations, region growing, K-NN classification, moment invariants   Performance: 83% average accuracy, 30 FPS single object, 15 FPS multiple objects    </p>"},{"location":"#camera-calibration-augmented-reality","title":"Camera Calibration &amp; Augmented Reality","text":"<p>Camera calibration and AR system projecting virtual 3D objects onto real-world scenes.</p> <p>Key Features: Zhang's calibration, pose estimation (solvePnP), 5 virtual objects   Concepts: Camera intrinsics/extrinsics, lens distortion, 3D-to-2D projection, pose estimation   Extensions: ArUco markers, OpenGL 3D models, homography-based projection    </p>"},{"location":"#character-recognition-deep-learning","title":"Character Recognition (Deep Learning)","text":"<p>CNN-based character recognition for handwritten digits and Greek letters with transfer learning.</p> <p>Key Features: LeNet-5 inspired CNN, transfer learning, filter visualization   Concepts: Convolutional Neural Networks, backpropagation, transfer learning, dropout regularization   Performance: 98%+ MNIST accuracy, 90%+ Greek letters with minimal training data    </p>"},{"location":"#repository-structure","title":"Repository Structure","text":"<pre><code>CS-5330-Pattern-Recognition-and-Computer-Vision/\n\u251c\u2500\u2500 Assignment1/          # Real-Time Filtering\n\u251c\u2500\u2500 Assignment2/          # Content-Based Image Retrieval\n\u251c\u2500\u2500 Assignment3/          # Real-Time 2D Object Recognition\n\u251c\u2500\u2500 Assignment4/          # Camera Calibration and AR\n\u2514\u2500\u2500 Assignment5/          # Character Recognition (Deep Learning)\n</code></pre> <p>Each assignment directory contains: - Source code implementations - Detailed README with approach and results - Sample images and test data - Build instructions and usage guides</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Each project has its own detailed documentation page (use navigation above). Generally:</p> <ol> <li>Prerequisites: OpenCV, C++ compiler (for assignments 1-4) or PyTorch (for assignment 5)</li> <li>Build: Follow CMake instructions in each assignment's README</li> <li>Run: Execute the compiled binary or Python script</li> <li>Documentation: Click on project links above for comprehensive guides</li> </ol>"},{"location":"#contact","title":"Contact","text":"<p>Author: Praphul Samavedam   GitHub: @PraphulSamavedam </p>"},{"location":"camera-calibration-ar/","title":"Camera Calibration and Augmented Reality","text":"<p>\u2190 Back to Home</p>"},{"location":"camera-calibration-ar/#overview","title":"Overview","text":"<p>This project implements a Camera Calibration and Augmented Reality (AR) system that calibrates cameras using a chessboard pattern and overlays virtual 3D objects onto real-world scenes in real-time. The system uses computer vision techniques to detect calibration targets, estimate camera pose, and project virtual content that accurately aligns with the physical environment.</p> <p>The system provides multiple executables for camera calibration, real-time AR projection, feature detection, and advanced AR extensions including ArUco marker tracking and OpenGL 3D model rendering.</p>"},{"location":"camera-calibration-ar/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   CAMERA CALIBRATION &amp; AUGMENTED REALITY                    \u2502\n\u2502                            System Workflow Diagram                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502    CALIBRATION PHASE             \u2502\n                   \u2502  (Offline - One Time Setup)      \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502 Video Stream (Camera Feed)  \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502  For Each Frame:             \u2502\n                   \u2502  1. Detect Chessboard        \u2502\n                   \u2502     (9\u00d76 internal corners)   \u2502\n                   \u2502  2. Extract Corner Points    \u2502\n                   \u2502  3. Refine using cornerSubPix\u2502\n                   \u2502  4. Display Detected Corners \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u2502 User presses 's'\n                                  \u25bc\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502  Store Calibration Data:     \u2502\n                   \u2502  - Image Points (2D corners) \u2502\n                   \u2502  - Object Points (3D world)  \u2502\n                   \u2502  (Repeat 5+ times)           \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u2502 User presses 'c'\n                                  \u25bc\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502  Camera Calibration:         \u2502\n                   \u2502  cv::calibrateCamera()       \u2502\n                   \u2502  - Minimize reprojection err \u2502\n                   \u2502  - Solve for intrinsics      \u2502\n                   \u2502  - Solve for distortion      \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502  Store to cameraParams.csv   \u2502\n                   \u2502  - Camera Matrix (3\u00d73)       \u2502\n                   \u2502  - Distortion Coefficients   \u2502\n                   \u2502  - Reprojection Error        \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                                                     \u2502\n        \u25bc                                                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 AR PROJECTION PHASE\u2502                          \u2502  EXTENSION PHASE   \u2502\n\u2502 (Real-time)        \u2502                          \u2502  (Advanced AR)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                                               \u2502\n         \u25bc                                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Load Camera Params \u2502                       \u2502 ArUco Marker AR      \u2502\n\u2502 from CSV           \u2502                       \u2502 OpenGL 3D Models     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u2502 Static Image/Video   \u2502\n         \u2502                                   \u2502 Homography-based     \u2502\n         \u25bc                                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  For Each Frame:               \u2502\n\u2502  1. Detect Chessboard          \u2502\n\u2502  2. Extract Corners            \u2502\n\u2502  3. Estimate Camera Pose       \u2502\n\u2502     - solvePnP (rotation)      \u2502\n\u2502     - solvePnP (translation)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Define Virtual Object:      \u2502\n\u2502  - 3D World Coordinates      \u2502\n\u2502  - Object type:              \u2502\n\u2502    - House ('h')             \u2502\n\u2502    - Rectangle+Axes ('r')    \u2502\n\u2502    - Arrow ('a')             \u2502\n\u2502    - Cone ('c')              \u2502\n\u2502    - Tetrahedron ('t')       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Project to Image Plane:     \u2502\n\u2502  cv::projectPoints()         \u2502\n\u2502  - Apply camera matrix       \u2502\n\u2502  - Apply distortion coeffs   \u2502\n\u2502  - Transform 3D \u2192 2D         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Draw Virtual Object:        \u2502\n\u2502  - Connect projected points  \u2502\n\u2502  - Draw lines/polygons       \u2502\n\u2502  - Add 3D axes (optional)    \u2502\n\u2502  - Render with perspective   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Display Results:            \u2502\n\u2502  - Original frame            \u2502\n\u2502  - Detected chessboard       \u2502\n\u2502  - Augmented reality overlay \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nKey Algorithms:\n\u251c\u2500 Chessboard Detection: cv::findChessboardCorners\n\u251c\u2500 Corner Refinement: cv::cornerSubPix\n\u251c\u2500 Camera Calibration: cv::calibrateCamera\n\u251c\u2500 Pose Estimation: cv::solvePnP\n\u251c\u2500 3D-to-2D Projection: cv::projectPoints\n\u2514\u2500 Harris Corner Detection: cv::cornerHarris\n</code></pre>"},{"location":"camera-calibration-ar/#key-features","title":"Key Features","text":""},{"location":"camera-calibration-ar/#core-capabilities","title":"Core Capabilities","text":"<ul> <li>Camera Calibration - Compute intrinsic camera parameters from chessboard images</li> <li>Real-time AR - Project virtual 3D objects onto detected calibration targets</li> <li>Pose Estimation - Calculate camera position and orientation relative to target</li> <li>Virtual Object Library - Multiple pre-defined 3D objects (house, axes, arrow, cone, tetrahedron)</li> <li>Harris Corner Detection - Feature extraction for advanced computer vision applications</li> <li>Static Image/Video AR - Project virtual objects onto recorded media</li> </ul>"},{"location":"camera-calibration-ar/#camera-calibration-features","title":"Camera Calibration Features","text":"<p>The system computes a complete camera model including:</p> <ol> <li>Camera Matrix (Intrinsic Parameters) - 3\u00d73 matrix encoding:</li> <li>Focal lengths (fx, fy)</li> <li>Principal point (cx, cy)</li> <li> <p>Skew coefficient (usually 0)</p> </li> <li> <p>Distortion Coefficients - 5 parameters modeling lens distortion:</p> </li> <li>k1, k2: Radial distortion (barrel/pincushion)</li> <li>p1, p2: Tangential distortion</li> <li> <p>k3: Higher-order radial distortion</p> </li> <li> <p>Reprojection Error - Quality metric for calibration accuracy (lower is better)</p> </li> </ol>"},{"location":"camera-calibration-ar/#virtual-objects","title":"Virtual Objects","text":"<p>The system can render the following 3D virtual objects:</p> <ul> <li>House (<code>'h'</code>) - 3D house structure with base and roof</li> <li>Rectangle with Axes (<code>'r'</code>) - Outer rectangle boundary with 3D coordinate axes (X, Y, Z)</li> <li>Arrow (<code>'a'</code>) - 3D arrow shape pointing upward</li> <li>Cone (<code>'c'</code>) - 3D cone with circular base</li> <li>Tetrahedron (<code>'t'</code>) - 3D pyramid with triangular base</li> </ul>"},{"location":"camera-calibration-ar/#technical-details","title":"Technical Details","text":""},{"location":"camera-calibration-ar/#camera-calibration-algorithm","title":"Camera Calibration Algorithm","text":"<p>The calibration process uses Zhang's Method (1998) to compute camera parameters:</p> <ol> <li>Chessboard Detection - Detect 9\u00d76 internal corners using <code>cv::findChessboardCorners</code></li> <li>Corner Refinement - Sub-pixel accuracy using <code>cv::cornerSubPix</code> with 5\u00d75 window</li> <li>Build Point Correspondences:</li> <li>Image Points: 2D pixel coordinates of detected corners</li> <li>Object Points: 3D world coordinates (assuming chessboard at Z=0 plane)</li> <li>Collect Multiple Views - Gather 5-15 image-object point pairs from different angles</li> <li>Optimize Parameters - Use <code>cv::calibrateCamera</code> to minimize reprojection error</li> </ol> <p>Mathematical Formulation: <pre><code>Minimize: \u03a3 ||image_points - projectPoints(object_points, rvec, tvec, K, distCoeffs)||\u00b2\n\nWhere:\n  K = Camera matrix [fx, 0, cx; 0, fy, cy; 0, 0, 1]\n  distCoeffs = [k1, k2, p1, p2, k3]\n  rvec, tvec = Rotation and translation vectors for each view\n</code></pre></p>"},{"location":"camera-calibration-ar/#pose-estimation-solvepnp","title":"Pose Estimation (solvePnP)","text":"<p>Once calibrated, the system estimates camera pose in real-time:</p> <ol> <li>Input: 3D object points, 2D image points, camera parameters</li> <li>Output: Rotation vector (rvec) and translation vector (tvec)</li> <li>Method: Iterative Levenberg-Marquardt optimization</li> <li>Use: Determines where virtual objects should be placed</li> </ol> <p>Geometric Interpretation: - rvec: How much the chessboard is rotated relative to camera (axis-angle representation) - tvec: Where the chessboard is located in camera space (tx, ty, tz)</p>"},{"location":"camera-calibration-ar/#3d-to-2d-projection-pipeline","title":"3D-to-2D Projection Pipeline","text":"<p>Virtual objects are projected using <code>cv::projectPoints</code>:</p> <pre><code>Step 1: World \u2192 Camera Coordinates\n    P_camera = R * P_world + tvec\n\nStep 2: Camera \u2192 Normalized Image Coordinates\n    x_norm = P_camera.x / P_camera.z\n    y_norm = P_camera.y / P_camera.z\n\nStep 3: Apply Lens Distortion\n    x_distorted = x_norm * (1 + k1*r\u00b2 + k2*r\u2074 + k3*r\u2076) + ...\n    y_distorted = y_norm * (1 + k1*r\u00b2 + k2*r\u2074 + k3*r\u2076) + ...\n\nStep 4: Normalized \u2192 Pixel Coordinates\n    u = fx * x_distorted + cx\n    v = fy * y_distorted + cy\n</code></pre>"},{"location":"camera-calibration-ar/#harris-corner-detection","title":"Harris Corner Detection","text":"<p>For feature extraction, the system implements Harris corner detection:</p> <pre><code>1. Compute image gradients: Ix, Iy\n2. Build structure tensor: M = [\u03a3(Ix\u00b2), \u03a3(Ix*Iy); \u03a3(Ix*Iy), \u03a3(Iy\u00b2)]\n3. Compute corner response: R = det(M) - k*trace(M)\u00b2\n4. Threshold and non-maximum suppression\n</code></pre>"},{"location":"camera-calibration-ar/#benefits-and-limitations","title":"Benefits and Limitations","text":"<p>Benefits</p> <ul> <li>High Accuracy: Reprojection errors &lt; 0.5 pixels achievable with proper calibration</li> <li>Real-time Performance: 30+ FPS on modern hardware</li> <li>Flexible: Works with any calibrated camera (webcams, smartphones, industrial cameras)</li> <li>Extensible: Easy to add new virtual objects and tracking methods</li> <li>Robust: Multiple tracking options (chessboard, ArUco markers, homography)</li> <li>Educational: Clear demonstration of camera calibration and AR principles</li> </ul> <p>Limitations</p> <ul> <li>Chessboard Dependency: Requires visible calibration target for basic AR mode</li> <li>Lighting Sensitive: Poor lighting affects chessboard detection accuracy</li> <li>Single Plane: Virtual objects anchored to planar target (no SLAM)</li> <li>No Occlusion Handling: Virtual objects don't hide behind real objects</li> <li>Calibration Required: Each camera needs individual calibration</li> <li>Limited Interaction: Virtual objects are static, not interactive</li> </ul>"},{"location":"camera-calibration-ar/#usage","title":"Usage","text":""},{"location":"camera-calibration-ar/#1-camera-calibration","title":"1. Camera Calibration","text":"<p>Calibrate your camera to compute intrinsic parameters:</p> <pre><code># macOS/Linux\n./main\n</code></pre> <p>Calibration Workflow: 1. Position 9\u00d76 chessboard pattern in front of camera 2. Wait for detection (green circles indicate successful detection) 3. Press 's' to save frame (repeat 5-10 times from different angles) 4. Vary angles by tilting, rotating, and moving the chessboard 5. Press 'c' to calibrate using all captured frames 6. Check reprojection error (aim for &lt; 1.0 pixels) 7. Press 'q' to quit and save parameters</p> <p>Output: Creates <code>resources/cameraParams.csv</code>: <pre><code>cameraMatrix,1303.0435,0.0000,184.4379,0.0000,1383.8424,103.2846,0.0000,0.0000,1.0000\ndistCoeff,-0.3816,10.6530,-0.0511,0.0079,-87.8130\nreprojectionError,0.3992\n</code></pre></p>"},{"location":"camera-calibration-ar/#2-real-time-ar-projection","title":"2. Real-time AR Projection","text":"<p>Project virtual objects onto the chessboard:</p> <pre><code># macOS/Linux\n./project\n</code></pre> <p>Controls: - Press 'q' to quit - Virtual object automatically renders when chessboard detected - Three windows display: Original, Detected, Augmented</p> <p>Customization: Edit <code>src/project.cpp</code>: - Line 25: Change <code>virtual_object</code> ('h', 'r', 'a', 'c', 't') - Line 29: Update <code>paramsFile</code> path if needed</p>"},{"location":"camera-calibration-ar/#3-harris-corner-detection","title":"3. Harris Corner Detection","text":"<p>Detect and visualize Harris corners in real-time:</p> <pre><code>./features\n</code></pre> <p>Output: - Original camera feed - Detected corners highlighted with circles</p>"},{"location":"camera-calibration-ar/#4-static-imagevideo-ar","title":"4. Static Image/Video AR","text":"<p>Apply AR to static media:</p> <pre><code># Static image AR\n./extensions resources/cameraParams.csv i resources/staticImage_1.jpg h\n\n# Static video AR\n./extensions resources/cameraParams.csv v resources/staticVid_1.mp4 a\n</code></pre> <p>Parameters: - <code>cameraParamsPath</code> - Path to calibration CSV - <code>mode</code> - <code>i</code> (image), <code>v</code> (video), <code>l</code> (live) - <code>staticFilePath</code> - Path to image/video file - <code>virtualObject</code> - Object type: <code>h</code>, <code>r</code>, <code>a</code>, <code>c</code>, <code>t</code></p>"},{"location":"camera-calibration-ar/#5-aruco-marker-ar","title":"5. ArUco Marker AR","text":"<p>Track ArUco markers instead of chessboards:</p> <pre><code>./multiTarget\n</code></pre> <p>Features: - Detects multiple markers simultaneously - More robust in complex environments - Uses DICT_6X6_250 dictionary</p>"},{"location":"camera-calibration-ar/#6-opengl-3d-model-rendering","title":"6. OpenGL 3D Model Rendering","text":"<p>Render complex 3D models using OpenGL:</p> <pre><code>./openGLExtension\n</code></pre> <p>Features: - Loads Suzanne 3D model (Blender monkey head) - Hardware-accelerated rendering (OpenGL 3.3) - Real-time lighting and shading</p>"},{"location":"camera-calibration-ar/#performance-metrics","title":"Performance Metrics","text":""},{"location":"camera-calibration-ar/#calibration-quality-guidelines","title":"Calibration Quality Guidelines","text":"Reprojection Error Quality Action &lt; 0.5 pixels Excellent Ready for AR 0.5 - 1.0 pixels Good Acceptable for most uses 1.0 - 2.0 pixels Acceptable Consider recalibration &gt; 2.0 pixels Poor Recalibrate with more images"},{"location":"camera-calibration-ar/#system-performance","title":"System Performance","text":"<ul> <li>Frame Rate: 30-60 FPS (depends on camera and processing)</li> <li>Calibration Time: 2-5 minutes (including image capture)</li> <li>Detection Range: 30cm to 100cm from camera</li> <li>Angle Tolerance: \u00b160\u00b0 from perpendicular view</li> <li>Lighting Requirements: Even, diffuse lighting without glare</li> </ul>"},{"location":"camera-calibration-ar/#learning-objectives","title":"Learning Objectives","text":"<p>This project demonstrates understanding of:</p> <ol> <li>Camera Geometry - Pinhole camera model, intrinsic/extrinsic parameters</li> <li>Camera Calibration - Zhang's method, reprojection error minimization</li> <li>Lens Distortion - Radial and tangential distortion models</li> <li>Pose Estimation - PnP problem, rotation representations</li> <li>3D Projections - World-to-image coordinate transformations</li> <li>Feature Detection - Corner detection (chessboard, Harris)</li> <li>Augmented Reality - Virtual object overlay with proper perspective</li> <li>Real-time Processing - Video stream processing, interactive applications</li> <li>Computer Vision Pipeline - Detection \u2192 Estimation \u2192 Projection \u2192 Rendering</li> <li>Extensions - ArUco markers, OpenGL rendering, homography methods</li> </ol>"},{"location":"camera-calibration-ar/#building-from-source","title":"Building from Source","text":"<p>For detailed build instructions, dependencies, and troubleshooting, see the Development Guide.</p> <p>Requirements: - OpenCV 4.x with ArUco module - GLFW, GLEW, GLM, Assimp (for extensions) - C++11 or later compiler - CMake or Makefile build system</p>"},{"location":"camera-calibration-ar/#references","title":"References","text":"<ul> <li>OpenCV Camera Calibration Tutorial</li> <li>Zhang's Camera Calibration Method (1998)</li> <li>OpenCV Pose Estimation (solvePnP)</li> <li>ArUco Marker Detection</li> </ul> <p>\u2190 Back to Home</p>"},{"location":"character-recognition/","title":"Character Recognition with Deep Learning","text":"<p>\u2190 Back to Home</p>"},{"location":"character-recognition/#overview","title":"Overview","text":"<p>This project implements a Character Recognition System using deep Convolutional Neural Networks (CNNs) built with PyTorch. The system recognizes handwritten digits from the MNIST dataset and extends to recognize Greek letters through transfer learning. The project explores various CNN architectures, training strategies, and visualization techniques to understand how deep neural networks learn to recognize characters.</p> <p>The system achieves 98%+ accuracy on MNIST digit recognition and successfully transfers learned features to recognize Greek letters (90%+ accuracy) with minimal additional training.</p>"},{"location":"character-recognition/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   CHARACTER RECOGNITION WITH DEEP LEARNING                  \u2502\n\u2502                              System Workflow Diagram                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502     BASE TRAINING PHASE          \u2502\n                    \u2502  (MNIST Digit Recognition)       \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 MNIST Dataset (70K images)   \u2502\n                    \u2502 - Train: 60,000 images       \u2502\n                    \u2502 - Test: 10,000 images        \u2502\n                    \u2502 - Size: 28\u00d728 grayscale      \u2502\n                    \u2502 - Classes: 0-9 (10 digits)   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Neural Network Architecture \u2502\n                    \u2502  (BaseNetwork - LeNet-5)     \u2502\n                    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n                    \u2502  \u2502 Conv1: 1\u219210, 5\u00d75       \u2502  \u2502\n                    \u2502  \u2502 MaxPool: 2\u00d72           \u2502  \u2502\n                    \u2502  \u2502 ReLU                   \u2502  \u2502\n                    \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502\n                    \u2502  \u2502 Conv2: 10\u219220, 5\u00d75      \u2502  \u2502\n                    \u2502  \u2502 Dropout2d: p=0.5       \u2502  \u2502\n                    \u2502  \u2502 MaxPool: 2\u00d72           \u2502  \u2502\n                    \u2502  \u2502 ReLU                   \u2502  \u2502\n                    \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502\n                    \u2502  \u2502 Flatten: 320 features  \u2502  \u2502\n                    \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502\n                    \u2502  \u2502 FC1: 320\u219250            \u2502  \u2502\n                    \u2502  \u2502 ReLU                   \u2502  \u2502\n                    \u2502  \u2502 FC2: 50\u219210             \u2502  \u2502\n                    \u2502  \u2502 LogSoftmax             \u2502  \u2502\n                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Training Loop (5 Epochs)    \u2502\n                    \u2502  \u251c\u2500 Forward Pass             \u2502\n                    \u2502  \u251c\u2500 Compute Loss (NLLLoss)   \u2502\n                    \u2502  \u251c\u2500 Backward Pass            \u2502\n                    \u2502  \u251c\u2500 Update Weights (SGD)     \u2502\n                    \u2502  \u2502   \u2022 Learning Rate: 0.01   \u2502\n                    \u2502  \u2502   \u2022 Momentum: 0.5         \u2502\n                    \u2502  \u2502   \u2022 Batch Size: 64        \u2502\n                    \u2502  \u2514\u2500 Log Progress             \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Save Trained Model          \u2502\n                    \u2502  models/final_model.pth      \u2502\n                    \u2502  \u2022 Model weights             \u2502\n                    \u2502  \u2022 Test accuracy: 98%+       \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                                                      \u2502\n        \u25bc                                                      \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 TRANSFER LEARNING  \u2502                           \u2502  EXPLORATION PHASE \u2502\n\u2502 (Greek Letters)    \u2502                           \u2502  (Architecture)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                                                \u2502\n         \u25bc                                                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Load Pretrained Model         \u2502              \u2502 Network Variations \u2502\n\u2502  models/final_model.pth        \u2502              \u2502 \u251c\u2500 Kernel Size     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502 \u251c\u2500 Filter Count    \u2502\n         \u2502                                      \u2502 \u251c\u2500 Network Depth   \u2502\n         \u25bc                                      \u2502 \u251c\u2500 Dropout Rate    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502 \u251c\u2500 Batch Size      \u2502\n\u2502  Freeze Convolutional Layers   \u2502              \u2502 \u2514\u2500 Optimizers      \u2502\n\u2502  \u2022 Conv1, Conv2: frozen        \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502  \u2022 FC layers: trainable        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Modify Last Layer             \u2502\n\u2502  \u2022 50\u219210 becomes 50\u21923          \u2502\n\u2502  \u2022 For 3 Greek letters         \u2502\n\u2502  \u2022 OR 50\u21927 for 7 letters       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Greek Dataset                 \u2502\n\u2502  \u2022 Alpha, Beta, Gamma          \u2502\n\u2502  \u2022 27 images per class         \u2502\n\u2502  \u2022 Transformed to 28\u00d728        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Fine-tune Training            \u2502\n\u2502  \u2022 15-45 epochs                \u2502\n\u2502  \u2022 Small learning rate (0.03)  \u2502\n\u2502  \u2022 Small batch size (5-6)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Save Greek Model              \u2502\n\u2502  models/model_greek.pth        \u2502\n\u2502  models/model_extended_greek   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nKey Algorithms:\n\u251c\u2500 Convolutional Neural Networks: Feature extraction\n\u251c\u2500 Transfer Learning: Reuse learned features\n\u251c\u2500 Stochastic Gradient Descent: Weight optimization\n\u251c\u2500 Backpropagation: Gradient computation\n\u251c\u2500 Dropout: Regularization to prevent overfitting\n\u2514\u2500 Data Augmentation: GreekTransform for preprocessing\n</code></pre>"},{"location":"character-recognition/#key-features","title":"Key Features","text":""},{"location":"character-recognition/#core-capabilities","title":"Core Capabilities","text":"<ul> <li>MNIST Digit Recognition - Train CNN from scratch (98%+ accuracy)</li> <li>Transfer Learning - Adapt pre-trained model to Greek letters (90%+ accuracy)</li> <li>Network Architecture Exploration - Compare different CNN designs and hyperparameters</li> <li>Filter Visualization - Understand what convolutional filters learn</li> <li>Custom Image Testing - Test on user-provided images via file dialog</li> <li>Training Monitoring - Real-time loss and accuracy visualization</li> </ul>"},{"location":"character-recognition/#neural-network-architectures","title":"Neural Network Architectures","text":"<p>The system implements multiple CNN architectures:</p> <ol> <li>BaseNetwork (LeNet-5 inspired) - Standard architecture with 10 and 20 filters</li> <li>Conv1: 1\u219210 filters, 5\u00d75 kernel</li> <li>MaxPool: 2\u00d72</li> <li>Conv2: 10\u219220 filters, 5\u00d75 kernel</li> <li>Dropout2d: p=0.5</li> <li>MaxPool: 2\u00d72</li> <li>FC1: 320\u219250</li> <li>FC2: 50\u219210 (or 3/7 for Greek)</li> <li> <p>Total Parameters: 21,840</p> </li> <li> <p>NetWorkKernel1 - Uses 1\u00d71 kernels instead of 5\u00d75</p> </li> <li>ManyParallelFiltersNetWork - Configurable filter counts</li> <li>DeepNetwork1 - Adds one extra convolution per layer</li> <li>DeepNetwork2 - Adds two extra convolutions per layer</li> </ol>"},{"location":"character-recognition/#transfer-learning-approach","title":"Transfer Learning Approach","text":"<p>Feature Extraction Strategy: 1. Freeze pretrained convolutional layers (Conv1, Conv2) 2. Replace final layer (10\u21923 or 10\u21927 classes) 3. Fine-tune only the new final layer 4. Requires minimal training data (27 images per class)</p> <p>Benefits: - 99% reduction in trainable parameters - Trains in ~45 seconds (vs. hours from scratch) - Works with limited data (81 images for 3 classes) - Leverages edge/curve/shape features from MNIST</p>"},{"location":"character-recognition/#technical-details","title":"Technical Details","text":""},{"location":"character-recognition/#cnn-architecture-basenetwork","title":"CNN Architecture (BaseNetwork)","text":"<pre><code>Input: 28\u00d728\u00d71 grayscale image\n\nLayer 1: Convolutional Feature Extraction\n\u251c\u2500 Conv2d(1\u219210, kernel=5\u00d75) \u2192 24\u00d724\u00d710 (edge detectors)\n\u251c\u2500 MaxPool2d(2\u00d72, stride=2) \u2192 12\u00d712\u00d710 (downsample)\n\u2514\u2500 ReLU() \u2192 Non-linearity\n\nLayer 2: Higher-Level Features\n\u251c\u2500 Conv2d(10\u219220, kernel=5\u00d75) \u2192 8\u00d78\u00d720 (combined features)\n\u251c\u2500 Dropout2d(p=0.5) \u2192 Regularization\n\u251c\u2500 MaxPool2d(2\u00d72, stride=2) \u2192 4\u00d74\u00d720\n\u2514\u2500 ReLU()\n\nFlatten: 4\u00d74\u00d720 \u2192 320-dimensional vector\n\nLayer 3: Classification\n\u251c\u2500 Linear(320\u219250) \u2192 Abstract representations\n\u251c\u2500 ReLU()\n\u251c\u2500 Linear(50\u219210) \u2192 Class scores\n\u2514\u2500 LogSoftmax() \u2192 Log probabilities\n\nTotal Parameters: 21,840\n</code></pre>"},{"location":"character-recognition/#training-algorithm","title":"Training Algorithm","text":"<p>Forward Pass: <pre><code>Input [B, 1, 28, 28] \u2192 Conv1 \u2192 Pool \u2192 ReLU \u2192\nConv2 \u2192 Dropout \u2192 Pool \u2192 ReLU \u2192 Flatten \u2192\nFC1 \u2192 ReLU \u2192 FC2 \u2192 LogSoftmax \u2192 Output [B, 10]\n</code></pre></p> <p>Backward Pass (SGD with Momentum): <pre><code>1. Compute loss: NLLLoss(predictions, labels)\n2. Compute gradients: loss.backward()\n3. Update weights: velocity = \u03bc\u00d7velocity - \u03b7\u00d7gradient\n                  weight = weight + velocity\n\nWhere:\n  \u03b7 = learning rate (0.01)\n  \u03bc = momentum (0.5)\n</code></pre></p> <p>Optimization Details: - Learning Rate: 0.01 (MNIST), 0.03 (Greek transfer) - Momentum: 0.5 (accelerates convergence) - Batch Size: 64 (MNIST), 5-6 (Greek) - Loss Function: NLLLoss (Negative Log Likelihood) - Regularization: Dropout (p=0.5)</p>"},{"location":"character-recognition/#transfer-learning-process","title":"Transfer Learning Process","text":"<pre><code># Step 1: Load pretrained model\nmodel = BaseNetwork()\nmodel.load_state_dict(torch.load(\"models/final_model.pth\"))\n\n# Step 2: Freeze convolutional layers\nfor param in model.convolution_stack.parameters():\n    param.requires_grad = False\n\n# Step 3: Replace final layer\nmodel.classification_stack[-2] = nn.Linear(50, num_classes)\n\n# Step 4: Fine-tune (train only final layer)\noptimizer = SGD(filter(lambda p: p.requires_grad, model.parameters()),\n                lr=0.03, momentum=0.1)\n</code></pre>"},{"location":"character-recognition/#greektransform-preprocessing","title":"GreekTransform Preprocessing","text":"<p>Converts Greek letter images (128\u00d7128 RGB) to MNIST format (28\u00d728 grayscale):</p> <pre><code>1. RGB \u2192 Grayscale: weighted sum (0.299\u00d7R + 0.587\u00d7G + 0.114\u00d7B)\n2. Affine Scaling: scale factor 36/128\n3. Center Crop: crop to 28\u00d728 from center\n4. Invert Colors: 255 - pixel (white-on-black \u2192 black-on-white)\n5. Normalize: (pixel/255 - 0.1307) / 0.3801\n</code></pre>"},{"location":"character-recognition/#benefits-and-limitations","title":"Benefits and Limitations","text":"<p>Benefits</p> <ul> <li>High Accuracy: 98%+ on MNIST, 90%+ on Greek letters</li> <li>Fast Training: 5 epochs (~10 minutes) for MNIST base model</li> <li>Transfer Learning: Achieves 90%+ with only 27 images per class</li> <li>Minimal Data: Greek model trained with 81 total images</li> <li>Interpretable: Filter visualization shows learned features</li> <li>Extensible: Easy to add new architectures and experiments</li> <li>Well-Documented: Comprehensive code with visualization tools</li> </ul> <p>Limitations</p> <ul> <li>Fixed Input Size: Requires 28\u00d728 images</li> <li>Grayscale Only: No color information used</li> <li>Limited Augmentation: Basic transformations only</li> <li>No Real-time: Not optimized for live video</li> <li>CPU Training: No GPU optimization in code</li> <li>Single Font: Greek letters must match training style</li> <li>Memory Usage: Larger batch sizes require significant RAM</li> </ul>"},{"location":"character-recognition/#usage","title":"Usage","text":""},{"location":"character-recognition/#1-training-on-mnist-digits","title":"1. Training on MNIST Digits","text":"<p>Train the base CNN model on MNIST:</p> <pre><code>python src/train_basic.py\n</code></pre> <p>Default hyperparameters: - Epochs: 5 - Learning rate: 0.01 - Momentum: 0.5 - Batch size: 64</p> <p>Custom hyperparameters: <pre><code>python src/train_basic.py -e 10 -r 0.02 -m 0.9 -br 128\n</code></pre></p> <p>Command-line arguments: - <code>-e, --epochs</code> - Number of training epochs - <code>-r, --rate</code> - Learning rate - <code>-m, --momentum</code> - SGD momentum - <code>-br, --train_batch_size</code> - Training batch size</p> <p>Output: - <code>models/final_model.pth</code> - Trained model weights - <code>base_network.png</code> - Network architecture visualization - Training curves showing loss vs. examples seen</p>"},{"location":"character-recognition/#2-testing-the-trained-model","title":"2. Testing the Trained Model","text":"<p>Evaluate the trained model on MNIST test data:</p> <pre><code>python src/test_basic.py\n</code></pre> <p>Output: - Visual display of predictions on 9 random test samples - Console output with prediction accuracy</p>"},{"location":"character-recognition/#3-transfer-learning-3-greek-letters","title":"3. Transfer Learning (3 Greek Letters)","text":"<p>Adapt the MNIST model to recognize Alpha, Beta, Gamma:</p> <pre><code>python src/transfer_greek.py\n</code></pre> <p>Parameters: - Epochs: 15 - Learning rate: 0.03 - Batch size: 5 - Classes: 3 (\u03b1, \u03b2, \u03b3)</p> <p>Output: - <code>models/model_greek.pth</code> - Transfer learned model - <code>greek_network.png</code> - Modified architecture - Training accuracy plots</p>"},{"location":"character-recognition/#4-transfer-learning-7-greek-letters","title":"4. Transfer Learning (7 Greek Letters)","text":"<p>Recognize 7 Greek letter classes:</p> <pre><code>python src/extended_greek.py\n</code></pre> <p>Parameters: - Epochs: 45 - Classes: 7 (\u03b1, \u03b2, \u03b3, \u03b7, \u03b4, \u03b8, \u03c6)</p> <p>Output: - <code>models/model_extended_greek.pth</code> - Extended model - Accuracy and loss curves</p>"},{"location":"character-recognition/#5-testing-greek-models","title":"5. Testing Greek Models","text":"<pre><code># Test 3-letter model\npython src/transfer_greek_testing.py\n\n# Test 7-letter model\npython src/extended_greek_testing.py\n</code></pre>"},{"location":"character-recognition/#6-custom-image-testing","title":"6. Custom Image Testing","text":"<p>Test on your own handwritten digit images:</p> <pre><code>python src/user_testing.py\n</code></pre> <p>What it does: 1. Opens file dialog for image selection 2. Allows multiple image selection 3. Preprocesses (resize, grayscale, normalize) 4. Predicts digit for each image 5. Displays results with accuracy</p> <p>Image requirements: - Any size (will be resized to 28\u00d728) - PNG, JPG, or other PIL-supported formats - Filename: <code>&lt;digit&gt;_description.png</code> (e.g., <code>5_myhandwriting.png</code>)</p>"},{"location":"character-recognition/#7-network-architecture-exploration","title":"7. Network Architecture Exploration","text":"<p>Explore different architectures and hyperparameters:</p> <pre><code>python src/explore_cnn.py\n</code></pre> <p>Experiments: - Filter size variation (1\u00d71, 5\u00d75, 9\u00d79) - Network depth comparison - Filter count variation</p>"},{"location":"character-recognition/#8-extensions-and-advanced-experiments","title":"8. Extensions and Advanced Experiments","text":"<pre><code>python src/extensions.py\n</code></pre> <p>Experiments: - Batch size variation (32, 64, 128, 512) - Dropout rate variation (0.0-1.0) - Optimizer comparison (SGD, Adam, Adagrad, Adadelta) - Fashion-MNIST transfer learning</p>"},{"location":"character-recognition/#9-filter-visualization","title":"9. Filter Visualization","text":"<p>Visualize learned convolutional filters:</p> <pre><code>python src/visualize_layers.py\n</code></pre> <p>Output: - <code>results/layer_0_filters.png</code> - Visualization of 10 Conv1 filters - <code>results/Information learned in layer 0.png</code> - Filter responses - <code>results/model_weights.log</code> - Detailed filter weights</p>"},{"location":"character-recognition/#performance-metrics","title":"Performance Metrics","text":""},{"location":"character-recognition/#mnist-digit-recognition","title":"MNIST Digit Recognition","text":"Metric Value Training Accuracy 98-99% Test Accuracy 98-99% Training Time (5 epochs) ~5-10 minutes (CPU) Reprojection Error N/A Model Size ~90KB"},{"location":"character-recognition/#greek-letter-recognition-transfer-learning","title":"Greek Letter Recognition (Transfer Learning)","text":"Dataset Classes Training Images Accuracy Training Time 3 Letters \u03b1, \u03b2, \u03b3 81 (27 each) 90-92% ~45 seconds 7 Letters +\u03b7, \u03b4, \u03b8, \u03c6 ~189 85-95% ~2 minutes"},{"location":"character-recognition/#architecture-comparison","title":"Architecture Comparison","text":"Network Kernel Depth Parameters Accuracy Time/Epoch BaseNetwork 5\u00d75 2 conv 21,840 98.5% ~2 min Kernel1 1\u00d71 2 conv 54,510 96.0% ~1.5 min DeepNetwork1 5\u00d75 4 conv 24,360 98.8% ~3 min DeepNetwork2 5\u00d75 6 conv 26,880 98.9% ~4 min <p>Insight: BaseNetwork offers best speed/accuracy trade-off for MNIST.</p>"},{"location":"character-recognition/#learning-objectives","title":"Learning Objectives","text":"<p>This project demonstrates understanding of:</p> <ol> <li>Deep Learning Fundamentals - Neural network architectures, forward/backward propagation</li> <li>Convolutional Neural Networks - Conv layers, pooling, feature extraction</li> <li>Training Strategies - SGD with momentum, learning rate, batch size</li> <li>Regularization - Dropout for preventing overfitting</li> <li>Transfer Learning - Feature extraction, layer freezing, fine-tuning</li> <li>Loss Functions - NLLLoss, LogSoftmax</li> <li>Data Preprocessing - Normalization, transformations</li> <li>Hyperparameter Tuning - Grid search, performance comparison</li> <li>Model Evaluation - Accuracy, confusion matrices, visualization</li> <li>PyTorch Framework - Model definition, training loops, data loaders</li> </ol>"},{"location":"character-recognition/#installation","title":"Installation","text":"<p>Requirements: - Python 3.7+ - PyTorch - torchvision - matplotlib - Pillow (PIL) - torchviz - OpenCV (optional)</p> <p>Quick install: <pre><code>pip install torch torchvision matplotlib pillow torchviz opencv-python\n</code></pre></p> <p>For detailed installation and troubleshooting, see the Development Guide.</p>"},{"location":"character-recognition/#references","title":"References","text":"<ul> <li>LeNet-5 Paper (LeCun et al., 1998)</li> <li>PyTorch Documentation</li> <li>Transfer Learning Tutorial</li> <li>Understanding CNNs (CS231n)</li> <li>Dropout Paper (Srivastava et al., 2014)</li> </ul> <p>\u2190 Back to Home</p>"},{"location":"image-retrieval/","title":"Content-Based Image Retrieval (CBIR) System","text":""},{"location":"image-retrieval/#overview","title":"Overview","text":"<p>The Content-Based Image Retrieval (CBIR) system is a sophisticated image search engine that finds and ranks similar images from a database based on visual features rather than text metadata. The system analyzes visual characteristics such as color distribution, texture patterns, and spatial layout to identify images with similar content.</p> <p>The implementation provides both a GUI-based interface for interactive exploration and a command-line interface for automated processing and integration into workflows.</p> <p>Key Concept</p> <p>Unlike traditional text-based image search, CBIR uses visual content analysis to find similar images. You query with an image, not keywords, making it ideal for visual search applications, duplicate detection, and content organization.</p>"},{"location":"image-retrieval/#system-architecture","title":"System Architecture","text":"<p>The CBIR system follows a multi-stage pipeline that efficiently processes queries through feature extraction, distance computation, and result ranking:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          CONTENT-BASED IMAGE RETRIEVAL                      \u2502\n\u2502                              Query Flow Diagram                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502 Query Image  \u2502\n                              \u2502 (Target)     \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                                     \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Select Feature Technique      \u2502\n                    \u2502  (Baseline, 3DHistogram, etc.) \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Extract Target Features       \u2502\n                    \u2502  \u2192 Feature Vector (e.g., 512D) \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 |\n                                 \u2502\n                                 \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Check Corresponding CSV Cache \u2502\n                    \u2502 (E.g. 3DHistogram.csv)        \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u2502 Found?\n                                 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500Yes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502                                \u2502\n                                 No                               \u2502\n                                 \u2502                                \u2502\n                                 \u2502                                \u2502\n                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n                      \u2502 Image Database        \u2502                   \u2502\n                      \u2502 (olympus/, testDB/)   \u2502                   \u2502\n                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n                                 \u2502                                \u2502\n                                 \u25bc                                |\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n                    | Process All DB Images    \u2502                  \u2502\n                    \u2502 \u251c\u2500 Read Image            \u2502                  \u2502\n                    \u2502 \u251c\u2500 Extract Features      \u2502                  \u2502\n                    \u2502 \u2514\u2500 Store in CSV          \u2502                  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n                               \u2502                                  |\n                               \u2514\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                      \u2502 Read Features    \u2502\n                      \u2502 from CSV (fast!) \u2502\n                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502 Database Features Loaded        \u2502\n              \u2502 (N images \u00d7 Feature Dimensions) \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502 Select Distance Metric          \u2502\n              \u2502 (HistogramError, SSE, etc.)     \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502 Compute Distances               \u2502\n              \u2502 For Each DB Image:              \u2502\n              \u2502   distance[i] = metric(         \u2502\n              \u2502     target_features,            \u2502\n              \u2502     db_features[i]              \u2502\n              \u2502   )                             \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502 Rank by Distance                \u2502\n              \u2502 Sort ascending (lower = similar)\u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502 Select Top-K Similar Images     \u2502\n              \u2502 (User specified: 3-15 images)   \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502 Display Results                 \u2502\n              \u2502 \u251c\u2500 Target Image                 \u2502\n              \u2502 \u251c\u2500 Similar Image 1 (dist: 0.12) \u2502\n              \u2502 \u251c\u2500 Similar Image 2 (dist: 0.18) \u2502\n              \u2502 \u2514\u2500 Similar Image K (dist: 0.25) \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                      \u2502              \u2502\n                      \u25bc              \u25bc\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 Save Results \u2502   \u2502 Exit (q)    \u2502\n            \u2502 Press 's'    \u2502   \u2502             \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nPerformance Note: First Run = Minutes | Cached Runs = Seconds\n</code></pre>"},{"location":"image-retrieval/#pipeline-stages","title":"Pipeline Stages","text":"<ol> <li>Feature Extraction: Converts images into numerical feature vectors that capture visual characteristics</li> <li>Feature Caching: Stores computed features in CSV files for ~100x speedup on subsequent queries</li> <li>Distance Computation: Compares query image features against database using selected metric</li> <li>Result Ranking: Sorts images by similarity (lower distance = more similar)</li> <li>Result Display: Shows top-K most similar images with distance scores</li> </ol>"},{"location":"image-retrieval/#key-features-and-capabilities","title":"Key Features and Capabilities","text":""},{"location":"image-retrieval/#core-capabilities","title":"Core Capabilities","text":"<ul> <li>Multiple Feature Extraction Methods: 8+ techniques for different visual characteristics</li> <li>Flexible Distance Metrics: 6 different similarity measures for various use cases</li> <li>Intelligent Caching: CSV-based feature storage reduces query time from minutes to seconds</li> <li>Dual Interface: Both GUI and command-line options for different workflows</li> <li>Scalable Performance: Handles databases with 1000+ images efficiently</li> <li>Result Persistence: Save retrieved images with descriptive filenames</li> </ul>"},{"location":"image-retrieval/#feature-extraction-techniques","title":"Feature Extraction Techniques","text":"<p>The system implements multiple complementary approaches to capture different aspects of visual similarity:</p>"},{"location":"image-retrieval/#1-baseline-99-center-pixels","title":"1. Baseline (9\u00d79 Center Pixels)","text":"<p>Extracts raw pixel values from the center 9\u00d79 region of the image.</p> <ul> <li>Vector Size: 243 features (81 pixels \u00d7 3 RGB channels)</li> <li>Best For: Aligned/registered images where objects are consistently positioned</li> <li>Limitations: Sensitive to translation, rotation, and scale variations</li> </ul> <p>How it works: <pre><code>center_row = height / 2\ncenter_col = width / 2\nfeature[i] = image[center_row - 4 : center_row + 5,\n                   center_col - 4 : center_col + 5]\n</code></pre></p>"},{"location":"image-retrieval/#2-2d-histogram-rg-chromaticity","title":"2. 2D Histogram (rg Chromaticity)","text":"<p>Uses normalized color distribution in rg chromaticity space, removing intensity information.</p> <ul> <li>Vector Size: 256 features (16\u00d716 bins)</li> <li>Best For: Color matching under varying lighting conditions</li> <li>Advantages: Illumination invariant</li> </ul> <p>Formula: <pre><code>r = R / (R + G + B + \u03b5)\ng = G / (R + G + B + \u03b5)\n</code></pre></p> <p>Where \u03b5 prevents division by zero.</p>"},{"location":"image-retrieval/#3-3d-histogram-rgb-color-space","title":"3. 3D Histogram (RGB Color Space)","text":"<p>Captures full color distribution across all three color channels simultaneously.</p> <ul> <li>Vector Size: 512 features (8\u00d78\u00d78 bins)</li> <li>Best For: Finding images with similar overall color composition</li> <li>Use Cases: Sunset photos, ocean images, fruit images</li> </ul> <p>Binning: <pre><code>r_index = R / 32  (256 values \u2192 8 bins)\ng_index = G / 32\nb_index = B / 32\nhistogram[r_index][g_index][b_index]++\n</code></pre></p>"},{"location":"image-retrieval/#4-multi-histogram-upper-bottom-split","title":"4. Multi-Histogram (Upper-Bottom Split)","text":"<p>Divides image horizontally and computes separate histograms for spatial awareness.</p> <ul> <li>Vector Size: 1024 features (2 \u00d7 512)</li> <li>Best For: Images with consistent vertical layout (sky/ground, head/body)</li> <li>Preserves: Vertical spatial relationships</li> </ul>"},{"location":"image-retrieval/#5-multi-histogram-left-right-split","title":"5. Multi-Histogram (Left-Right Split)","text":"<p>Divides image vertically to preserve horizontal spatial layout.</p> <ul> <li>Vector Size: 1024 features (2 \u00d7 512)</li> <li>Best For: Horizontally structured scenes, objects positioned left/right</li> <li>Preserves: Horizontal spatial relationships</li> </ul>"},{"location":"image-retrieval/#6-texture-color-tac","title":"6. Texture &amp; Color (TAC)","text":"<p>Combines gradient magnitude histograms with color histograms for rich feature representation.</p> <ul> <li>Vector Size: Variable (gradient histogram + color histogram)</li> <li>Best For: Distinguishing textured vs smooth surfaces</li> <li>Components: 50% Sobel gradient texture + 50% RGB color</li> </ul> <p>Gradient Computation: <pre><code>Sobel_X = [[-1, 0, 1],    Sobel_Y = [[-1, -2, -1],\n           [-2, 0, 2],               [ 0,  0,  0],\n           [-1, 0, 1]]               [ 1,  2,  1]]\n\nmagnitude = sqrt(Sobel_X\u00b2 + Sobel_Y\u00b2)\n</code></pre></p>"},{"location":"image-retrieval/#7-q4-texture-histogram-four-quarters","title":"7. Q4 Texture Histogram (Four Quarters)","text":"<p>Divides image into four quadrants with texture information for fine-grained spatial awareness.</p> <ul> <li>Vector Size: 2048+ features (4 quadrants \u00d7 512 + texture)</li> <li>Best For: Spatially structured scenes (faces, layouts)</li> <li>Preserves: 2D spatial color distribution</li> </ul> <p>Quadrant Division: <pre><code>[Top-Left]    [Top-Right]\n[Bottom-Left] [Bottom-Right]\n</code></pre></p>"},{"location":"image-retrieval/#8-custom-histogram-center-weighted-texture","title":"8. Custom Histogram (Center-Weighted + Texture)","text":"<p>Focuses on central region with weighted combination of color and edge features.</p> <ul> <li>Vector Size: Variable</li> <li>Best For: Centered objects (portraits, product images)</li> <li>Weighting: 65% center color + 35% Laplacian edge texture</li> </ul> <p>Distance Formula: <pre><code>distance = 0.65 \u00d7 color_distance + 0.35 \u00d7 texture_distance\n</code></pre></p>"},{"location":"image-retrieval/#additional-specialized-techniques","title":"Additional Specialized Techniques","text":"<ul> <li>Specific Color Detection: Targeted detection for yellow/banana tones, blue bins, green bins</li> <li>Edge and Color Combination: Canny edges with color histograms</li> <li>Two Halves with rg Chromaticity: Spatial split using illumination-invariant colors</li> </ul>"},{"location":"image-retrieval/#distance-metrics","title":"Distance Metrics","text":"<p>The system provides multiple distance metrics to compare feature vectors, each suited for different feature types:</p>"},{"location":"image-retrieval/#1-sum-of-squared-error-sse-aggsquareerror","title":"1. Sum of Squared Error (SSE) - <code>AggSquareError</code>","text":"<p>Classic Euclidean distance squared between feature vectors.</p> <pre><code>distance = \u03a3(feature1[i] - feature2[i])\u00b2\n</code></pre> <ul> <li>Properties: Simple, fast, magnitude-sensitive</li> <li>Best For: Baseline technique, features with similar scales</li> <li>Range: [0, \u221e)</li> </ul>"},{"location":"image-retrieval/#2-histogram-intersection-error-histogramerror","title":"2. Histogram Intersection Error - <code>HistogramError</code>","text":"<p>Measures overlap between normalized histograms.</p> <pre><code>intersection = \u03a3 min(histogram1[i], histogram2[i])\ndistance = 1 - intersection\n</code></pre> <ul> <li>Properties: Robust to partial occlusion, bounded output</li> <li>Best For: All histogram-based features (RGB, rg, multi-region)</li> <li>Range: [0, 1]</li> </ul>"},{"location":"image-retrieval/#3-entropy-error-entropyerror","title":"3. Entropy Error - <code>EntropyError</code>","text":"<p>Compares information content and complexity of feature distributions.</p> <pre><code>entropy(H) = -\u03a3 H[i] \u00d7 log(H[i])\ndistance = |entropy(H1) - entropy(H2)|\n</code></pre> <ul> <li>Properties: Measures distribution complexity similarity</li> <li>Best For: Texture complexity comparison, pattern diversity</li> <li>Interpretation: Low entropy = uniform/smooth, High entropy = diverse/textured</li> </ul>"},{"location":"image-retrieval/#4-weighted-histogram-error-80-20-w82histogramerror","title":"4. Weighted Histogram Error (80-20) - <code>W82HistogramError</code>","text":"<p>Allows different importance weights for combined feature components.</p> <pre><code>distance = 1 - \u03a3 weight[j] \u00d7 intersection_error(section_j)\n</code></pre> <ul> <li>Properties: Flexible component weighting</li> <li>Best For: Combined features (e.g., 80% color + 20% texture)</li> <li>Use Case: Color-dominant vs texture-dominant matching</li> </ul>"},{"location":"image-retrieval/#5-mean-square-error-mse-meansquareerror","title":"5. Mean Square Error (MSE) - <code>MeanSquareError</code>","text":"<p>Normalized version of SSE, independent of feature vector length.</p> <pre><code>distance = sqrt(mean((feature1[i] - feature2[i])\u00b2))\n</code></pre> <ul> <li>Properties: Normalized by vector length, scale-independent</li> <li>Best For: Comparing different feature types, mixed features</li> <li>Advantage: Less sensitive to dimensionality differences</li> </ul>"},{"location":"image-retrieval/#6-masked-boundary-error-maskedbounderror","title":"6. Masked Boundary Error - <code>MaskedBoundError</code>","text":"<p>Specialized metric for comparing segmented regions and object shapes.</p> <pre><code>overlap = min(area1, area2) + min(ratio1, ratio2)\ndistance = 2 - overlap\n</code></pre> <ul> <li>Properties: Geometric similarity measure</li> <li>Best For: Shape-based matching, segmented object comparison</li> <li>Features: Normalized contour area + aspect ratio</li> </ul>"},{"location":"image-retrieval/#feature-metric-matching-guide","title":"Feature-Metric Matching Guide","text":"Scenario Recommended Feature Recommended Metric Color-based objects (fruits, clothing) 3D RGB Histogram Histogram Intersection Different lighting conditions rg Chromaticity Histogram Intersection Texture-rich images (fabrics, patterns) Texture &amp; Color (TAC) Entropy Error Spatially consistent scenes (landscapes) Upper-Bottom Histogram Histogram Intersection Centered objects (portraits, products) Custom (Center-Weighted) Weighted Histogram Raw pixel matching (aligned images) Baseline 9\u00d79 Sum of Squared Error Object shape similarity Any + segmentation Masked Boundary Error"},{"location":"image-retrieval/#benefits-and-limitations","title":"Benefits and Limitations","text":"<p>Benefits</p> <p>Efficient Performance</p> <ul> <li>CSV caching provides ~100x speedup: first run in minutes, cached runs in seconds</li> <li>Scalable to databases with 1000+ images</li> <li>Parallel-friendly architecture for batch processing</li> </ul> <p>Flexible Feature Extraction</p> <ul> <li>8+ feature techniques capture different visual characteristics</li> <li>Supports color, texture, spatial layout, and combined approaches</li> <li>Illumination-invariant options (rg chromaticity)</li> </ul> <p>Multiple Distance Metrics</p> <ul> <li>6 distance functions for different similarity criteria</li> <li>Histogram intersection robust to partial occlusion</li> <li>Weighted combinations for multi-modal features</li> </ul> <p>User-Friendly Interface</p> <ul> <li>Interactive GUI for exploration and experimentation</li> <li>Command-line interface for automation and scripting</li> <li>Visual result display with distance scores</li> </ul> <p>Research and Learning</p> <ul> <li>Demonstrates fundamental computer vision techniques</li> <li>Modular design for extending with new features</li> <li>Educational value for understanding CBIR systems</li> </ul> <p>Limitations</p> <p>Computational Constraints</p> <ul> <li>First-time feature extraction takes minutes for large databases</li> <li>Memory usage scales with database size and feature dimensionality</li> <li>Real-time performance requires pre-computed features</li> </ul> <p>Feature Limitations</p> <ul> <li>Color histograms lose spatial information (except multi-region variants)</li> <li>Texture features sensitive to image resolution and quality</li> <li>Baseline technique requires object alignment</li> </ul> <p>Invariance Challenges</p> <ul> <li>Most features not rotation or scale invariant</li> <li>Significant viewpoint changes affect matching</li> <li>Background clutter can interfere with object-focused queries</li> </ul> <p>Database Requirements</p> <ul> <li>Requires manual cache reset when database contents change</li> <li>No automatic change detection for database updates</li> <li>CSV cache files must match current database state</li> </ul> <p>Semantic Gap</p> <ul> <li>Visual similarity \u2260 semantic similarity</li> <li>Cannot understand high-level concepts (e.g., \"happy person\")</li> <li>No relevance feedback or query refinement in current implementation</li> </ul> <p>Optimization Strategies</p> <ul> <li>Use cached features whenever possible (avoid unnecessary <code>resetFile</code>)</li> <li>Choose appropriate feature-metric combinations for your use case</li> <li>Start with 3D Histogram + Histogram Intersection for general-purpose queries</li> <li>Use rg chromaticity for illumination-robust color matching</li> <li>Combine spatial techniques (multi-region) for layout-sensitive queries</li> <li>Consider texture features when color alone is insufficient</li> </ul>"},{"location":"image-retrieval/#usage-examples","title":"Usage Examples","text":""},{"location":"image-retrieval/#gui-interface","title":"GUI Interface","text":"<p>The graphical interface provides an interactive workflow for exploring different feature techniques and distance metrics.</p> <p>Basic Command: <pre><code>./guiMain &lt;targetImagePath&gt; &lt;imagesDatabasePath&gt;\n</code></pre></p> <p>Example: <pre><code>./guiMain data/testImg/pic.0164.jpg data/olympus\n</code></pre></p> <p>Interactive Workflow:</p> <ol> <li>Feature Selection Window opens first</li> <li>Click a button to select feature extraction technique</li> <li>Options: Baseline, 2D Histogram, 3D Histogram, Upper-Bottom, Left-Right, TAC, Q4 Texture, Custom</li> <li> <p>Optional: Check \"Re-evaluate ft vectors\" to force feature recomputation</p> </li> <li> <p>Distance &amp; Settings Window opens next</p> </li> <li>Use trackbar to select number of similar images (3-15)</li> <li>Click a button to select distance metric</li> <li> <p>Options: AggSquareError, HistogramError, EntropyError, W82HistogramError, MeanSquareError</p> </li> <li> <p>Results Display shows:</p> </li> <li>Query/target image (reference)</li> <li>Top-K similar images ranked by distance</li> <li> <p>Distance scores for each match</p> </li> <li> <p>Keyboard Controls:</p> </li> <li>Press <code>s</code> to save retrieved images with descriptive filenames</li> <li>Press <code>q</code> to quit the application</li> </ol> <p>Saved Filename Format: <pre><code>&lt;FeatureTechnique&gt;_&lt;DistanceMetric&gt;_&lt;Rank&gt;_&lt;OriginalFilename&gt;\n</code></pre></p> <p>Example: <code>3DHistogram_HistogramError_1_pic.0245.jpg</code></p>"},{"location":"image-retrieval/#command-line-interface","title":"Command-Line Interface","text":"<p>The command-line interface enables scripting, automation, and batch processing.</p> <p>Basic Command: <pre><code>./cmdMain &lt;targetImagePath&gt; &lt;imagesDatabasePath&gt; &lt;featureTechnique&gt; \\\n          &lt;distanceMetric&gt; &lt;numberOfSimilarImages&gt; [resetFile] [echoStatus]\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>targetImagePath</code> - Path to the query/target image</li> <li><code>imagesDatabasePath</code> - Directory containing the image database</li> <li><code>featureTechnique</code> - Feature extraction method (see list below)</li> <li><code>distanceMetric</code> - Distance calculation method (see list below)</li> <li><code>numberOfSimilarImages</code> - Number of similar images to retrieve (integer)</li> <li><code>resetFile</code> (optional) - Non-zero to force feature recomputation (default: 0)</li> <li><code>echoStatus</code> (optional) - Non-zero for verbose output (default: 0)</li> </ul> <p>Feature Technique Options:</p> <ul> <li><code>Baseline</code> - 9\u00d79 center pixel extraction</li> <li><code>2DHistogram</code> - rg chromaticity histogram</li> <li><code>3DHistogram</code> - RGB color histogram</li> <li><code>2HalvesUBHistogram</code> - Upper-Bottom multi-histogram</li> <li><code>2HalvesLRHistogram</code> - Left-Right multi-histogram</li> <li><code>TACHistogram</code> - Texture and color combination</li> <li><code>Q4TextureHistogram</code> - Four-quarters with texture</li> <li><code>CustomHistogram</code> - Center-weighted color with texture</li> </ul> <p>Distance Metric Options:</p> <ul> <li><code>AggSquareError</code> - Sum of squared error (SSE)</li> <li><code>HistogramError</code> - Histogram intersection</li> <li><code>EntropyError</code> - Entropy difference</li> <li><code>W82HistogramError</code> - Weighted 80-20 histogram</li> <li><code>MeanSquareError</code> - Mean square error (MSE)</li> <li><code>MaskedBoundError</code> - Masked boundary error</li> </ul> <p>Example 1: Basic Query <pre><code>./cmdMain data/testImg/pic.0164.jpg data/olympus 3DHistogram HistogramError 5\n</code></pre></p> <p>Retrieves 5 similar images using 3D RGB histogram features and histogram intersection metric.</p> <p>Example 2: Query with Feature Recomputation <pre><code>./cmdMain data/testImg/pic.0164.jpg data/olympus 3DHistogram HistogramError 5 1 0\n</code></pre></p> <p>Forces recomputation of all features (useful when database changed).</p> <p>Example 3: Verbose Query <pre><code>./cmdMain data/testImg/pic.0164.jpg data/olympus TACHistogram EntropyError 10 0 1\n</code></pre></p> <p>Retrieves 10 images using texture+color with verbose output for debugging.</p> <p>Example 4: Illumination-Robust Query <pre><code>./cmdMain data/testImg/sunset.jpg data/olympus 2DHistogram HistogramError 8\n</code></pre></p> <p>Uses rg chromaticity for lighting-invariant color matching.</p> <p>Example 5: Spatial-Aware Query <pre><code>./cmdMain data/testImg/landscape.jpg data/olympus 2HalvesUBHistogram HistogramError 7\n</code></pre></p> <p>Uses upper-bottom split to preserve vertical spatial layout (sky/ground).</p>"},{"location":"image-retrieval/#batch-processing-script-example","title":"Batch Processing Script Example","text":"<pre><code>#!/bin/bash\n# Query all images in a directory and save results\n\nTARGET_DIR=\"data/testImages\"\nDATABASE=\"data/olympus\"\nFEATURE=\"3DHistogram\"\nMETRIC=\"HistogramError\"\nNUM_RESULTS=5\n\nfor img in \"$TARGET_DIR\"/*.jpg; do\n    echo \"Processing: $img\"\n    ./cmdMain \"$img\" \"$DATABASE\" \"$FEATURE\" \"$METRIC\" \"$NUM_RESULTS\" 0 1\ndone\n</code></pre>"},{"location":"image-retrieval/#learning-objectives","title":"Learning Objectives","text":"<p>This project demonstrates fundamental concepts in computer vision and pattern recognition:</p>"},{"location":"image-retrieval/#1-feature-extraction-fundamentals","title":"1. Feature Extraction Fundamentals","text":"<ul> <li>Color Representation: Understanding RGB, rg chromaticity, and color spaces</li> <li>Histogram Techniques: Building and normalizing multi-dimensional histograms</li> <li>Texture Analysis: Gradient magnitude, Sobel filters, Laplacian edge detection</li> <li>Spatial Encoding: Preserving layout information through region-based features</li> </ul>"},{"location":"image-retrieval/#2-distance-metrics-and-similarity","title":"2. Distance Metrics and Similarity","text":"<ul> <li>Euclidean Distance: SSE and MSE for vector comparison</li> <li>Histogram Comparison: Intersection, entropy, and weighted methods</li> <li>Metric Selection: Matching distance functions to feature types</li> <li>Normalization: Handling features with different scales and ranges</li> </ul>"},{"location":"image-retrieval/#3-system-design-principles","title":"3. System Design Principles","text":"<ul> <li>Caching Strategy: Trading disk space for computation time (CSV storage)</li> <li>Modular Architecture: Separating feature extraction from distance computation</li> <li>Interface Design: Supporting both interactive (GUI) and automated (CLI) workflows</li> <li>Scalability: Handling databases from tens to thousands of images</li> </ul>"},{"location":"image-retrieval/#4-computer-vision-concepts","title":"4. Computer Vision Concepts","text":"<ul> <li>Content-Based Retrieval: Visual similarity vs semantic similarity</li> <li>Feature Trade-offs: Discriminative power vs computational cost</li> <li>Invariance Properties: Illumination, rotation, scale, and translation robustness</li> <li>The Semantic Gap: Limitations of low-level features for high-level concepts</li> </ul>"},{"location":"image-retrieval/#5-practical-skills","title":"5. Practical Skills","text":"<ul> <li>OpenCV Usage: Image I/O, color space conversion, filtering operations</li> <li>Performance Optimization: Feature caching, efficient data structures</li> <li>Experimental Methodology: Testing different feature-metric combinations</li> <li>Result Analysis: Interpreting distance scores and ranking quality</li> </ul>"},{"location":"image-retrieval/#6-algorithm-understanding","title":"6. Algorithm Understanding","text":"<ul> <li>Convolution Operations: Sobel and Laplacian filters for edge detection</li> <li>Statistical Summaries: Histograms as probability distributions</li> <li>Dimensionality: Trading feature vector size for discriminative power</li> <li>Weighting Schemes: Combining multiple feature types effectively</li> </ul>"},{"location":"image-retrieval/#technical-details","title":"Technical Details","text":""},{"location":"image-retrieval/#performance-characteristics","title":"Performance Characteristics","text":"<p>Feature Extraction Times (approximate, 1000 images):</p> <ul> <li>Baseline: ~30 seconds (simple pixel extraction)</li> <li>2D/3D Histogram: ~2-3 minutes (histogram computation)</li> <li>Texture features: ~4-5 minutes (gradient computation)</li> <li>Multi-region: ~3-4 minutes (multiple histograms)</li> </ul> <p>Query Times with Caching:</p> <ul> <li>Feature loading from CSV: &lt;2 seconds</li> <li>Distance computation (1000 images): &lt;1 second</li> <li>Total cached query time: &lt;3 seconds</li> <li>Speedup factor: ~100x faster than recomputation</li> </ul> <p>Memory Usage:</p> <ul> <li>3D Histogram (512 features): ~4KB per image</li> <li>1000 images: ~4MB feature storage</li> <li>Multi-region techniques: ~8MB for 1000 images</li> </ul>"},{"location":"image-retrieval/#feature-vector-dimensions","title":"Feature Vector Dimensions","text":"Feature Technique Vector Size Storage per Image Baseline (9\u00d79) 243 ~1 KB 2D Histogram (16\u00d716) 256 ~1 KB 3D Histogram (8\u00d78\u00d78) 512 ~2 KB Upper-Bottom Split 1024 ~4 KB Left-Right Split 1024 ~4 KB TAC (Texture + Color) ~600-800 ~3 KB Q4 Texture ~2100 ~8 KB Custom (Center + Texture) Variable ~2-3 KB"},{"location":"image-retrieval/#csv-cache-format","title":"CSV Cache Format","text":"<p>Features are stored in CSV format for efficient reuse:</p> <pre><code>filename,feature_0,feature_1,feature_2,...,feature_N\ndata/olympus/pic.0001.jpg,0.124,0.089,0.201,...,0.156\ndata/olympus/pic.0002.jpg,0.098,0.156,0.087,...,0.234\ndata/olympus/pic.0003.jpg,0.201,0.045,0.178,...,0.092\n</code></pre> <p>Each feature technique maintains its own CSV file: - <code>3DHistogram.csv</code> - RGB histogram features - <code>2DHistogram.csv</code> - rg chromaticity features - <code>Baseline.csv</code> - Center pixel features - And so on...</p>"},{"location":"image-retrieval/#error-codes","title":"Error Codes","text":"<ul> <li>Error -100: Insufficient command-line arguments</li> <li>Error -400: Image too small for baseline technique (requires &gt;9\u00d79)</li> <li>Error -1000: Fewer images found in database than requested</li> <li>Error -200: Cannot open/read target image file</li> <li>Error -300: Invalid feature technique or distance metric specified</li> </ul>"},{"location":"image-retrieval/#build-and-requirements","title":"Build and Requirements","text":""},{"location":"image-retrieval/#dependencies","title":"Dependencies","text":"<ul> <li>OpenCV 4.x - Core image processing library</li> <li>C++11 or later - Standard library features</li> <li>CVUI library - Simple OpenCV-based GUI components</li> </ul>"},{"location":"image-retrieval/#building-the-project","title":"Building the Project","text":"<p>For detailed build instructions, dependency installation, and troubleshooting, refer to:</p> <p>Assignment 2 DEVELOPMENT.md</p> <p>Quick build overview: <pre><code># Navigate to project directory\ncd Assignment2/Assignment2\n\n# Compile GUI version\ng++ -o guiMain guiMain.cpp features.cpp distance.cpp \\\n    `pkg-config --cflags --libs opencv4`\n\n# Compile command-line version\ng++ -o cmdMain cmdMain.cpp features.cpp distance.cpp \\\n    `pkg-config --cflags --libs opencv4`\n</code></pre></p>"},{"location":"image-retrieval/#supported-image-formats","title":"Supported Image Formats","text":"<p>The system supports all image formats handled by OpenCV: - JPEG (.jpg, .jpeg) - PNG (.png) - BMP (.bmp) - TIFF (.tif, .tiff) - WebP (.webp)</p> <p>Images of varying sizes are supported - the system handles resizing and normalization internally.</p>"},{"location":"image-retrieval/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements and research directions:</p> <p>Deep Learning Integration - CNN-based feature extraction (VGG, ResNet features) - Learned distance metrics - Transfer learning from pre-trained models</p> <p>Advanced Querying - Relevance feedback and query refinement - Multi-modal queries (combining multiple target images) - Weighted region-of-interest selection - Partial image matching</p> <p>Performance Optimization - GPU acceleration for feature extraction - Approximate nearest neighbor search (LSH, FAISS) - Incremental database updates - Parallel batch processing</p> <p>Additional Features - Real-time video query support - Web-based interface with REST API - Mobile application integration - Cloud-based deployment</p> <p>Enhanced Techniques - SIFT/SURF keypoint-based matching - Object segmentation integration - Multi-scale feature pyramids - Attention-based feature weighting</p> <p>\u2190 Back to Home</p>"},{"location":"object-recognition/","title":"Real-Time 2D Object Recognition (RT2DOR)","text":""},{"location":"object-recognition/#overview","title":"Overview","text":"<p>The Real-Time 2D Object Recognition (RT2DOR) system is a comprehensive computer vision solution that detects, segments, and classifies objects in both static images and live video streams. Built on classical computer vision techniques, the system combines image segmentation with K-Nearest Neighbors (K-NN) classification to recognize objects based on their geometric and spatial features.</p> <p>The system recognizes 17 different object classes and achieves 83% average accuracy on the test dataset. It provides multiple executables for different use cases: training classifiers, analyzing static images, detecting multiple objects, generating confusion matrices, and performing real-time video recognition.</p> <p>Demo Video: Watch the system in action</p>"},{"location":"object-recognition/#system-architecture","title":"System Architecture","text":"<p>The RT2DOR system follows a modular pipeline architecture with distinct training and recognition phases:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    REAL-TIME 2D OBJECT RECOGNITION                          \u2502\n\u2502                         System Workflow Diagram                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502     TRAINING PHASE               \u2502\n                    \u2502  (Offline - One Time Setup)      \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Training Images Folder       \u2502\n                    \u2502 (Labeled: Beanie, Cup, etc.) \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  For Each Training Image:    \u2502\n                    \u2502  1. Threshold (Binary)       \u2502\n                    \u2502  2. Morphology (Clean)       \u2502\n                    \u2502  3. Region Growing (Segment) \u2502\n                    \u2502  4. Extract Features         \u2502\n                    \u2502     - Area Ratio             \u2502\n                    \u2502     - Aspect Ratio           \u2502\n                    \u2502     - Percent Filled         \u2502\n                    \u2502     - Hu Moments (7 values)  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Store in features.csv       \u2502\n                    \u2502  Format:                     \u2502\n                    \u2502  path,label,feat1,...,feat9  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                                                     \u2502\n         \u25bc                                                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 RECOGNITION PHASE  \u2502                            \u2502  EVALUATION PHASE  \u2502\n\u2502 (Real-time/Static) \u2502                            \u2502  (Offline)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                                                  \u2502\n         \u25bc                                                  \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Input Source:      \u2502                         \u2502 Confusion Matrix     \u2502\n\u2502 - Live Video       \u2502                         \u2502 Generator            \u2502\n\u2502 - Static Image     \u2502                         \u2502 \u251c\u2500 Load features.csv \u2502\n\u2502 - Multiple Objects \u2502                         \u2502 \u251c\u2500 K-NN Predictions  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502 \u251c\u2500 Compare Labels    \u2502\n         \u2502                                     \u2502 \u2514\u2500 Generate Matrix   \u2502\n         \u25bc                                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Image Processing Pipeline:  \u2502\n\u2502  1. Threshold                \u2502\n\u2502     \u2514\u2500 Binary Image          \u2502\n\u2502  2. Dilation                 \u2502\n\u2502     \u2514\u2500 Fill Holes            \u2502\n\u2502  3. Erosion                  \u2502\n\u2502     \u2514\u2500 Remove Noise          \u2502\n\u2502  4. Region Growing           \u2502\n\u2502     \u2514\u2500 Connected Components  \u2502\n\u2502  5. Top N Segments           \u2502\n\u2502     \u2514\u2500 Filter by Area        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Feature Extraction          \u2502\n\u2502  For Each Detected Region:   \u2502\n\u2502  \u251c\u2500 Compute Moments          \u2502\n\u2502  \u251c\u2500 Calculate Dimensions     \u2502\n\u2502  \u251c\u2500 Extract Hu Moments       \u2502\n\u2502  \u2514\u2500 Generate Feature Vector  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  K-NN Classification         \u2502\n\u2502  1. Load features.csv        \u2502\n\u2502  2. Compute Distances        \u2502\n\u2502     - Euclidean (default)    \u2502\n\u2502     - Scaled Euclidean       \u2502\n\u2502  3. Find K Nearest Neighbors \u2502\n\u2502  4. Majority Vote for Label  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Output Results              \u2502\n\u2502  \u251c\u2500 Draw Bounding Box        \u2502\n\u2502  \u251c\u2500 Display Label            \u2502\n\u2502  \u251c\u2500 Show Confidence          \u2502\n\u2502  \u2514\u2500 Real-time Update         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nKey Algorithms Used:\n\u251c\u2500 Thresholding: Binary segmentation\n\u251c\u2500 Grass Fire Algorithm: Distance transform for morphology\n\u251c\u2500 Region Growing: 4/8-connected component labeling\n\u251c\u2500 Moment Invariants: Rotation/scale invariant features\n\u2514\u2500 K-NN Classifier: Distance-based classification\n</code></pre>"},{"location":"object-recognition/#features-and-capabilities","title":"Features and Capabilities","text":""},{"location":"object-recognition/#core-capabilities","title":"Core Capabilities","text":"<ol> <li>Training System - Automated feature extraction and database creation</li> <li>Real-time Recognition - Live video object detection and classification</li> <li>Static Image Analysis - Detailed analysis of single images with intermediate steps</li> <li>Multiple Object Detection - Simultaneous recognition of multiple objects in a scene</li> <li>Evaluation Tools - Confusion matrix generation and accuracy metrics</li> <li>Morphological Processing - Custom erosion, dilation, and region growing algorithms</li> </ol>"},{"location":"object-recognition/#key-highlights","title":"Key Highlights","text":"<ul> <li>17 Object Classes: Recognizes diverse categories including wearables, electronics, stationery, and household items</li> <li>83% Accuracy: Achieves strong classification performance across all classes</li> <li>Real-time Performance: Processes video at ~30 FPS for single objects</li> <li>Unknown Object Handling: Dynamically learns new objects through user interaction</li> <li>Custom Algorithms: Implements morphology and segmentation from scratch</li> </ul>"},{"location":"object-recognition/#supported-object-classes","title":"Supported Object Classes","text":"<p>The system is trained to recognize 17 different object types:</p> Category Objects Wearables Beanie, Cap, Glove, Mask, Watch Electronics Phone, Mic, Remote, Ear Buds, Fire Alarm Stationery Book, Pen, Wallet Household Spoon, Umbrella, Glasses Case, Bottle Cap, Chess Board <p>Unknown Object Handling</p> <p>When an object doesn't match any trained class (distance exceeds threshold), the system prompts the user to provide a new label and automatically adds it to the training database, enabling continuous learning.</p>"},{"location":"object-recognition/#feature-extraction-techniques","title":"Feature Extraction Techniques","text":""},{"location":"object-recognition/#9-dimensional-feature-vectors","title":"9-Dimensional Feature Vectors","text":"<p>The system extracts comprehensive feature vectors for each object:</p> <ol> <li>Area Ratio - Normalized object area relative to image size</li> <li>Aspect Ratio - Width-to-height ratio of oriented bounding box</li> <li>Percent Filled - Ratio of object pixels to bounding box area</li> <li>Hu Moments (7 values) - Rotation, scale, and translation invariant shape descriptors</li> </ol>"},{"location":"object-recognition/#hu-moment-invariants","title":"Hu Moment Invariants","text":"<p>Hu moments provide powerful shape descriptors that remain constant under geometric transformations:</p> <ul> <li>\u03c6\u2081: Captures general shape characteristics</li> <li>\u03c6\u2082: Measures elongation</li> <li>\u03c6\u2083: Captures triangularity</li> <li>\u03c6\u2084-\u03c6\u2087: Higher-order shape characteristics</li> </ul> <p>Mathematical Foundation:</p> <pre><code>Normalized Central Moments:\n\u03b7_pq = \u03bc_pq / (\u03bc\u2080\u2080)^((p+q)/2 + 1)\n\nFirst Hu Moment:\n\u03c6\u2081 = \u03b7\u2082\u2080 + \u03b7\u2080\u2082\n\nSecond Hu Moment:\n\u03c6\u2082 = (\u03b7\u2082\u2080 - \u03b7\u2080\u2082)\u00b2 + 4\u03b7\u2081\u2081\u00b2\n</code></pre> <p>Why Hu Moments?</p> <p>These features remain constant under rotation, scaling, and translation, making them ideal for robust object recognition across varying viewpoints and scales.</p>"},{"location":"object-recognition/#k-nn-classification-methods","title":"K-NN Classification Methods","text":""},{"location":"object-recognition/#algorithm-overview","title":"Algorithm Overview","text":"<p>The K-Nearest Neighbors classifier predicts object labels based on similarity to training samples:</p> <pre><code>1. Extract features from unknown object \u2192 query_vector\n2. Load all training samples \u2192 database\n3. For each training sample:\n       distance = euclidean_distance(query_vector, sample_vector)\n4. Sort by distance (ascending)\n5. Select K nearest neighbors\n6. Predicted_label = majority_vote(K_neighbor_labels)\n</code></pre>"},{"location":"object-recognition/#distance-metrics","title":"Distance Metrics","text":"Metric Formula Use Case Euclidean <code>\u221a\u03a3(a-b)\u00b2</code> Equal feature importance Scaled Euclidean <code>\u221a\u03a3w(a-b)\u00b2</code> Weighted features (prioritize specific features)"},{"location":"object-recognition/#choosing-k-value","title":"Choosing K Value","text":"K Value Best For Pros Cons K=1 Small, clean datasets Fast, simple Sensitive to noise/outliers K=3 General purpose Balanced, good noise reduction Slightly slower K=5 Large datasets (default) Robust to outliers Slower classification K=7+ Very noisy datasets Most stable May oversmooth, slowest <p>Tie-breaking Rule</p> <p>If multiple labels have the same vote count (e.g., 2 votes for \"Book\" and 2 votes for \"Phone\"), the system selects the label with the smallest average distance from the query object.</p>"},{"location":"object-recognition/#benefits-and-limitations","title":"Benefits and Limitations","text":"<p>Benefits</p> <ul> <li>Fast Training: Simple feature extraction enables quick model updates</li> <li>Interpretable: Clear feature-based classification easy to debug</li> <li>No GPU Required: Runs efficiently on standard hardware</li> <li>Scale/Rotation Invariant: Hu moments provide robust shape recognition</li> <li>Real-time Performance: 30 FPS for single objects, 15 FPS for multiple objects</li> <li>Continuous Learning: Unknown object handling enables dynamic expansion</li> </ul> <p>Limitations</p> <ul> <li>Lighting Sensitivity: Fixed threshold may fail under varying illumination</li> <li>Similar Shapes: Objects with similar geometry can be confused (e.g., Cap vs Bottle Cap)</li> <li>Occlusion: Partial visibility significantly reduces accuracy</li> <li>Background Assumption: Works best with high-contrast, clean backgrounds</li> <li>Database Size: Performance degrades with very large training sets (&gt;1000 samples)</li> <li>No Color/Texture: Uses only geometric features, missing discriminative information</li> </ul>"},{"location":"object-recognition/#usage-examples","title":"Usage Examples","text":""},{"location":"object-recognition/#1-training-the-classifier","title":"1. Training the Classifier","text":"<p>Train the system by providing labeled images:</p> WindowsLinux/macOS <pre><code># Automatic mode (labels from filenames)\nbin\\train.exe data\\train_images\n\n# Manual mode (prompt for labels)\nbin\\train.exe data\\train_images data\\db\\features.csv m\n</code></pre> <pre><code># Automatic mode (labels from filenames)\n./train data/train_images\n\n# Manual mode (prompt for labels)\n./train data/train_images data/db/features.csv m\n</code></pre> <p>Filename Format for Automatic Mode: <pre><code>ObjectName###.jpg\nExamples: Beanie01.jpg, Cup05.jpg, Phone10.jpg\n</code></pre></p> <p>Output: Creates <code>features.csv</code> with format: <pre><code>filepath,label,area_ratio,aspect_ratio,percent_filled,hu1,hu2,hu3,hu4,hu5,hu6,hu7\n</code></pre></p>"},{"location":"object-recognition/#2-real-time-video-recognition","title":"2. Real-time Video Recognition","text":"<p>Recognize objects in live video feed:</p> WindowsLinux/macOS <pre><code>bin\\Match.exe\n</code></pre> <pre><code>./vidDisplay\n</code></pre> <p>Controls:</p> <ul> <li>Press <code>q</code> to quit</li> <li>Press <code>s</code> to save current frame</li> <li>System displays recognized label with K-NN confidence</li> </ul>"},{"location":"object-recognition/#3-multiple-object-detection","title":"3. Multiple Object Detection","text":"<p>Detect and recognize multiple objects in a static image:</p> WindowsLinux/macOS <pre><code>bin\\MultipleObjects.exe data\\example001.png data\\db\\features.csv euclidean 3\n</code></pre> <pre><code>./multipleObjects data/example001.png data/db/features.csv euclidean 3\n</code></pre> <p>Parameters:</p> <ul> <li><code>imagePath</code> - Path to input image</li> <li><code>featuresFile</code> - Path to trained features CSV</li> <li><code>distanceMetric</code> - <code>euclidean</code> or <code>scaled_euclidean</code></li> <li><code>K</code> (optional) - Number of nearest neighbors (default: 1)</li> </ul>"},{"location":"object-recognition/#4-static-image-analysis","title":"4. Static Image Analysis","text":"<p>Analyze a single image with detailed intermediate steps:</p> WindowsLinux/macOS <pre><code>bin\\StaticImageAnalyzer.exe data\\example068.png\n</code></pre> <pre><code>./staticImageAnalyzer data/example068.png\n</code></pre> <p>Output: Displays intermediate images:</p> <ul> <li>Original image</li> <li>Binary (thresholded) image</li> <li>Cleaned image (after morphology)</li> <li>Segmented regions (color-coded)</li> <li>Final labeled image</li> </ul>"},{"location":"object-recognition/#5-confusion-matrix-generation","title":"5. Confusion Matrix Generation","text":"<p>Evaluate classifier performance:</p> WindowsLinux/macOS <pre><code>bin\\ConfusionMatrixGenerator.exe data\\db\\features.csv data\\db\\confusion_matrix.csv 3\n</code></pre> <pre><code>./confusionMatrix data/db/features.csv data/db/confusion_matrix.csv 3\n</code></pre> <p>How It Works:</p> <ol> <li>Loads all training samples from features CSV</li> <li>For each sample, performs leave-one-out cross-validation:</li> <li>Remove sample from training set</li> <li>Predict label using K-NN on remaining samples</li> <li>Compare prediction with ground truth label</li> <li>Builds confusion matrix: rows = actual labels, columns = predicted labels</li> <li>Calculates per-class and overall accuracy</li> </ol> <p>Interpreting Results:</p> <ul> <li>Diagonal values = correct predictions (higher is better)</li> <li>Off-diagonal values = misclassifications (identify confusion patterns)</li> <li>Right column shows per-class accuracy</li> <li>Bottom row shows overall system accuracy</li> </ul>"},{"location":"object-recognition/#performance-metrics","title":"Performance Metrics","text":""},{"location":"object-recognition/#system-accuracy","title":"System Accuracy","text":"<p>The system achieves 83% average accuracy across 17 object classes using K=5 and scaled Euclidean distance.</p> <p>Top Performing Classes:</p> Class Accuracy Glove 96.4% Beanie 92.3% Mask 91.7% Chess Board 90.0% Ear Buds 89.3% Remote 89.5% Spoon 88.9% Book 88.2% <p>Challenging Classes:</p> Class Accuracy Common Confusions Cap 65.0% Fire Alarm, Wallet Bottle Cap 69.2% Cap, Book Fire Alarm 60.0% Cap, Book Watch 70.0% Mic <p>Key Observations</p> <ul> <li>Classes with distinctive shapes (Glove, Beanie, Mask) achieve &gt;90% accuracy</li> <li>Similar-shaped objects (Cap vs Bottle Cap) show confusion patterns</li> <li>Training with 10-20 samples per class yields best results</li> </ul>"},{"location":"object-recognition/#real-time-performance","title":"Real-time Performance","text":"<p>Training Phase:</p> <ul> <li>Time Complexity: O(N \u00d7 M) where N = number of images, M = pixels per image</li> <li>Typical Time: 100 images (~2-5 minutes)</li> <li>Bottleneck: Region growing and moment computation</li> </ul> <p>Recognition Phase:</p> <ul> <li>Time Complexity: O(K \u00d7 D) where K = training samples, D = feature dimensions</li> <li>Single object: ~30 FPS (33ms per frame)</li> <li>Multiple objects (3-5): ~15 FPS (66ms per frame)</li> <li>Bottleneck: K-NN distance computation for large databases</li> </ul>"},{"location":"object-recognition/#optimization-tips","title":"Optimization Tips","text":"<p>Performance Optimization</p> <ol> <li>Reduce Database Size: Only keep diverse, representative samples</li> <li>Limit K Value: K=1 or K=3 is much faster than K=7+</li> <li>Lower Resolution: Scale down input images (e.g., 640\u00d7480 \u2192 320\u00d7240)</li> <li>Optimize Morphology: Reduce erosion/dilation iterations if acceptable</li> <li>Compile with Optimization: Use <code>-O3</code> flag for 20-30% speedup</li> </ol>"},{"location":"object-recognition/#learning-objectives","title":"Learning Objectives","text":"<p>This project demonstrates key computer vision and pattern recognition concepts:</p>"},{"location":"object-recognition/#core-algorithms-implemented","title":"Core Algorithms Implemented","text":"<ol> <li>Image Segmentation</li> <li>Binary thresholding (including Otsu's automatic method)</li> <li>Morphological operations (erosion, dilation)</li> <li> <p>Connected component labeling (region growing)</p> </li> <li> <p>Feature Extraction</p> </li> <li>Spatial moments computation</li> <li>Central moments (translation invariant)</li> <li>Hu moment invariants (rotation/scale/translation invariant)</li> <li> <p>Geometric features (area ratio, aspect ratio, percent filled)</p> </li> <li> <p>Classification</p> </li> <li>K-Nearest Neighbors algorithm</li> <li>Distance metrics (Euclidean, scaled Euclidean)</li> <li> <p>Majority voting with tie-breaking</p> </li> <li> <p>System Design</p> </li> <li>Training/testing pipeline separation</li> <li>Real-time video processing</li> <li>Performance evaluation (confusion matrix)</li> </ol>"},{"location":"object-recognition/#advanced-techniques","title":"Advanced Techniques","text":"<ul> <li>Grass Fire Algorithm: Custom implementation for distance transforms</li> <li>Stack-based Region Growing: Efficient connected component analysis</li> <li>Otsu Thresholding: Automatic threshold selection via variance maximization</li> <li>Unknown Object Handling: Dynamic database expansion through user interaction</li> </ul>"},{"location":"object-recognition/#practical-skills","title":"Practical Skills","text":"<ul> <li>OpenCV integration for image I/O and display</li> <li>CSV database management for feature storage</li> <li>Real-time video capture and processing</li> <li>Performance optimization and profiling</li> <li>Debugging computer vision pipelines</li> </ul>"},{"location":"object-recognition/#source-code-and-documentation","title":"Source Code and Documentation","text":"<p>The complete implementation, build instructions, and detailed algorithm explanations are available in the project repository:</p> <p>View Assignment 3 Source Code \u2192</p> <p>For building from source, development setup, and contribution guidelines, see:</p> <p>Development Documentation \u2192</p> <p>\u2190 Back to Home</p>"},{"location":"real-time-filtering/","title":"Assignment 1: Real-Time Filtering","text":"<p>A real-time image processing application that applies various visual effects to both static images and live camera feed.</p>"},{"location":"real-time-filtering/#demo","title":"Demo","text":""},{"location":"real-time-filtering/#overview","title":"Overview","text":"<p>This project demonstrates fundamental image processing techniques through a real-time video filtering application. The system allows users to apply various visual effects to live camera feed or static images, showcasing core computer vision concepts like color space manipulation, convolution operations, and real-time processing.</p>"},{"location":"real-time-filtering/#features","title":"Features","text":""},{"location":"real-time-filtering/#available-filters","title":"Available Filters","text":"<ul> <li>Negative Effect - Inverts image colors</li> <li>Grayscale - Converts to black and white</li> <li>Blur Filters - Various blur effects using convolution</li> <li>Cartoonization - Artistic cartoon-style rendering</li> <li>Edge Detection - Detects and highlights edges</li> <li>Custom Effects - User-defined filter combinations</li> </ul>"},{"location":"real-time-filtering/#capabilities","title":"Capabilities","text":"<ul> <li>Real-time video processing from webcam</li> <li>Static image processing from files</li> <li>Save processed images (press 's')</li> <li>Interactive filter switching</li> <li>Smooth performance for real-time applications</li> </ul>"},{"location":"real-time-filtering/#learning-objectives","title":"Learning Objectives","text":"<p>Through this assignment, the following concepts were explored:</p>"},{"location":"real-time-filtering/#color-space-exploration","title":"Color Space Exploration","text":"<ul> <li>RGB color space fundamentals</li> <li>HSV color space for hue/saturation manipulation</li> <li>Color channel separation and manipulation</li> <li>Colorspace conversion techniques</li> </ul>"},{"location":"real-time-filtering/#data-types-and-applications","title":"Data Types and Applications","text":"<ul> <li>Understanding image representation (8-bit, 16-bit, floating point)</li> <li>When to use different data types</li> <li>Precision vs. performance tradeoffs</li> <li>Type conversions and their implications</li> </ul>"},{"location":"real-time-filtering/#real-time-video-processing","title":"Real-time Video Processing","text":"<ul> <li>Capturing and processing video frames</li> <li>Frame rate considerations</li> <li>Buffering and memory management</li> <li>Performance optimization for real-time constraints</li> </ul>"},{"location":"real-time-filtering/#filter-applications","title":"Filter Applications","text":"<ul> <li>Convolution operations and kernels</li> <li>Spatial domain filtering</li> <li>Frequency domain concepts</li> <li>Filter design and implementation</li> </ul>"},{"location":"real-time-filtering/#technical-implementation","title":"Technical Implementation","text":""},{"location":"real-time-filtering/#core-technologies","title":"Core Technologies","text":"<ul> <li>OpenCV for image processing and video capture</li> <li>C++ for performance-critical operations</li> <li>Real-time processing pipeline design</li> </ul>"},{"location":"real-time-filtering/#key-algorithms","title":"Key Algorithms","text":"<ul> <li>Convolution-based filtering</li> <li>Color space transformations</li> <li>Edge detection (Sobel, Canny)</li> <li>Morphological operations</li> </ul>"},{"location":"real-time-filtering/#usage","title":"Usage","text":"<p>The application provides an interactive interface where users can:</p> <ol> <li>Start the application with a video source or image file</li> <li>Switch between different filters using keyboard controls</li> <li>View the effects in real-time</li> <li>Save interesting results for later use</li> </ol> <p>Press 'q' to quit the application at any time.</p>"},{"location":"real-time-filtering/#project-structure","title":"Project Structure","text":"<pre><code>Assignment1/\n\u251c\u2500\u2500 src/              # Source code\n\u251c\u2500\u2500 include/          # Header files\n\u251c\u2500\u2500 data/             # Sample images\n\u2514\u2500\u2500 README.md         # This file\n</code></pre>"},{"location":"real-time-filtering/#demonstration","title":"Demonstration","text":"<p>The demo video above shows the system applying various filters in real-time to live video feed, demonstrating the smooth performance and visual quality of the implemented effects.</p> <p>\u2190 Back to Home</p>"}]}